{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T12:02:48.454413Z",
     "start_time": "2020-03-19T12:02:48.383376Z"
    }
   },
   "source": [
    "created by Ignacio Oguiza - email: oguiza@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to efficiently work with very large Numpy Arrays?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "ISCOLAB = 'google.colab' in sys.modules\n",
    "if ISCOLAB:\n",
    "    if not os.path.isdir('/content/timeseries'):\n",
    "        !pip install git+https://github.com/fastai/fastai2 \n",
    "        !pip install git+https://github.com/fastai/fastcore \n",
    "        !pip install pyunpack\n",
    "        !pip install sktime\n",
    "        !git clone https://github.com/timeseriesAI/timeseriesAI2.git\n",
    "        %cd timeseries\n",
    "    else: \n",
    "        path = !pwd\n",
    "        if path != ['/content/timeseries']: \n",
    "            %cd timeseries\n",
    "        !pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I normally work with time series data. I made the decision to use numpy arrays to store my data since the can easily handle multiple dimensions, and are really very efficient.\n",
    "\n",
    "But sometimes datasets are really big (many GBs) and don't fit in memory. So I started looking around and found something that works very well: [**np.memmap**](https://docs.scipy.org/doc/numpy/reference/generated/numpy.memmap.html). Conceptually they work as arrays on disk, and that's how I often call them.\n",
    "\n",
    "np.memmap creates a map to numpy array you have previously saved on disk, so that you can efficiently access small segments of those (small or large) files on disk, without reading the entire file into memory. And that's exactly what we need with deep learning, be able to quickly create a batch in memory, without reading the entire file (that is stored on disk). \n",
    "\n",
    "The best analogy I've found are image files. You may have a very large dataset on disk (that far exceeds your RAM). In order to create your DL datasets, what you pass are the paths to each individual file, so that you can then load a few images and create a batch on demand.\n",
    "\n",
    "You can view np.memmap as the path collection that can be used to load numpy data on demand when you need to create a batch.\n",
    "\n",
    "So let's see how you can work with larger than RAM arrays on disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "On this laptop I have only 8GB of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RAM      :  8.00 GB\n",
      "Available RAM  :  2.19 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fastai2.torch_core import *\n",
    "from timeseries.imports import *\n",
    "from timeseries.utils import *\n",
    "print(f'Total RAM      : {bytes2GB(psutil.virtual_memory().total):5.2f} GB')\n",
    "print(f'Available RAM  : {bytes2GB(psutil.virtual_memory().available):5.2f} GB\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I will try to demonstratehow you can handle a 10 GB numpy array dataset in an efficient way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Create and save a larger than memory array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I will now create a large numpy array that doesn't fit in memory. \n",
    "Since I don't have enough RAM, I'll create an empty array on disk, and then load data in chunks that fit in memory.\n",
    "\n",
    "⚠️ If you want to to experiment with large datasets, you may uncomment and run this code. **It will create a ~10GB on your disk**. \n",
    "If you do it, remember to delete it later.\n",
    "In my laptop it took me around **11 mins to run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # Save a small empty array\n",
    "# X_temp_fn = 'temp_X.npy'\n",
    "# np.save(X_temp_fn, np.empty(1))\n",
    "\n",
    "# # Create a np.memmap with desired dtypes and shape of the large array you want to save.\n",
    "# # It's just a placeholder that doesn't contain any data\n",
    "# X_fn = 'X_on_disk.npy'\n",
    "# X = np.memmap(X_temp_fn, dtype='float32', shape=(100000, 50, 512))\n",
    "\n",
    "# # We are going to create a loop to fill in the np.memmap\n",
    "# start = 0\n",
    "# for i in range(20):\n",
    "#     # You now grab a chunk of your data that fits in memory\n",
    "#     # This could come from a pandas dataframe for example\n",
    "#     # I will simulate it with some random data\n",
    "#     data_chunk = np.random.rand(5000, 50, 512)\n",
    "#     end = start + data_chunk.shape[0]\n",
    "    \n",
    "#     # I now fill a slice of the np.memmap\n",
    "#     X[start:end] = data_chunk\n",
    "    \n",
    "#     start = end\n",
    "#     del data_chunk\n",
    "\n",
    "# #I can now remove the temp file I created\n",
    "# os.remove(X_temp_fn)\n",
    "\n",
    "# # Once the data is loaded on the np.memmap, I save it as a normal np.array\n",
    "# np.save(X_fn, X)\n",
    "\n",
    "# # I will create a smaller array. Sinc this fits in memory, I don't need to use a memmap\n",
    "# y_fn = 'y_on_disk.npy'\n",
    "# y = np.random.randint(0, 10, X.shape[0])\n",
    "# labels = np.array(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j'])\n",
    "# np.save(y_fn, labels[y])\n",
    "\n",
    "# del X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Ok. So let's check the size of these files on memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X array:  10240000128 bytes (9.540 GB)\n",
      "y array:       400128 bytes (0.000 GB)\n"
     ]
    }
   ],
   "source": [
    "print(f'X array: {os.path.getsize(\"X_on_disk.npy\"):12} bytes ({bytes2GB(os.path.getsize(\"X_on_disk.npy\")):3.3f} GB)')\n",
    "print(f'y array: {os.path.getsize(\"y_on_disk.npy\"):12} bytes ({bytes2GB(os.path.getsize(\"y_on_disk.npy\")):3.3f} GB)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load an array on disk (np.memmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Remember I only have an 8 GB RAM on this laptop, so I couldn't load these datasets in memory.\n",
    "\n",
    "☣️ Actually I accidentally loaded the \"X_on_disk.npy\" file, and my laptop crahsed so I had to reboot it!\n",
    "\n",
    "So let's now load data as arrays on disk (np.memmap). The way to do it is super simple, and very efficient. You just do it as you would with a normal array, but add an mmap_mode.\n",
    "\n",
    "There are 4 modes: \n",
    "\n",
    "- ‘r’\tOpen existing file for reading only.\n",
    "- ‘r+’\tOpen existing file for reading and writing.\n",
    "- ‘w+’\tCreate or overwrite existing file for reading and writing.\n",
    "- ‘c’\tCopy-on-write: assignments affect data in memory, but changes are not saved to disk. The file on disk is read-only.\n",
    "\n",
    "I normally use mode 'r' since I want to be able to make changes to data in memory (transforms for example), without affecting data on disk (same approach as with image data). This is the same thing you do with image files on disk, that are just read, and then modified in memory, without change the file on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_on_disk = np.load('X_on_disk.npy', mmap_mode='r')\n",
    "y_on_disk = np.load('y_on_disk.npy', mmap_mode='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Fast load**: it only takes a few ms to load a memory map to a 10 GB array on disk. Great!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Arrays on disk: main features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Very limited RAM usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X array on disk:          152 bytes (0.000 GB)\n",
      "y array on disk:          120 bytes (0.000 GB)\n"
     ]
    }
   ],
   "source": [
    "print(f'X array on disk: {sys.getsizeof(X_on_disk):12} bytes ({bytes2GB(sys.getsizeof(X_on_disk)):3.3f} GB)')\n",
    "print(f'y array on disk: {sys.getsizeof(y_on_disk):12} bytes ({bytes2GB(sys.getsizeof(y_on_disk)):3.3f} GB)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**152 bytes of RAM for a 10GB array**. This is the great benefit of arrays on disk.\n",
    "\n",
    "Arrays on disk barely use any RAM until each the it's sliced and an element is converted into a np.array or a tensor.\n",
    "\n",
    "This is equivalent to the size of file paths in images (very limited) compared to the files themselves (actual images). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.memmap"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_on_disk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(X_on_disk, np.ndarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**np.memmap are np.ndarrays** with a type np.memmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "With np.memmap you can perform the same operations you would with a normal numpy array. \n",
    "However, the most common operations you will perform are:\n",
    "\n",
    "- slicing\n",
    "- calculating stats: mean and std\n",
    "- conversion to a tensor\n",
    "\n",
    "Once you get the array on disk slice, you'll convert it into a tensor, move to a GPU and performs operations there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "⚠️ You need to be careful though not to convert the entire np.memmap to an array/ tensor if it's larger than your RAM. This will crash your computer unless you have enough RAM, so you would have to reboot!\n",
    "\n",
    "**DON'T DO THIS:  torch.from_numpy(X) or np.array(X)** unless you have ehough RAM.\n",
    "\n",
    "To avoid issues during test, I created a smaller array on disk (that I can store in memory). When I want to test something I test it with that array first. It's important to always verify that the type output of your operations is np.memmap, which means data is still in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If you use mode 'r' you can grab a sample and make changes to it, but this won't modify data on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([[0.2626543 , 0.81090355, 0.32464218, ..., 0.23839498, 0.67757344,\n",
       "         0.3310132 ],\n",
       "        [0.7845197 , 0.68123066, 0.38421404, ..., 0.5383686 , 0.01147997,\n",
       "         0.13235402],\n",
       "        [0.6679299 , 0.56707335, 0.02308166, ..., 0.77351165, 0.20462573,\n",
       "         0.9898304 ],\n",
       "        ...,\n",
       "        [0.38630033, 0.5770135 , 0.18095827, ..., 0.9798217 , 0.56657016,\n",
       "         0.6018766 ],\n",
       "        [0.09030128, 0.01747155, 0.5882474 , ..., 0.28344667, 0.6954193 ,\n",
       "         0.93599343],\n",
       "        [0.91863537, 0.01252496, 0.4146458 , ..., 0.80348647, 0.9133351 ,\n",
       "         0.5418618 ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = X_on_disk[0]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "output array is read-only",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9c5bb4848ea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: output array is read-only"
     ]
    }
   ],
   "source": [
    "x += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You can't. This is the behavior you want so that you never modify the original dataset. Thos operations are usually performed once the slice is converted into a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2627, 1.8109, 1.3246,  ..., 1.2384, 1.6776, 1.3310],\n",
       "        [1.7845, 1.6812, 1.3842,  ..., 1.5384, 1.0115, 1.1324],\n",
       "        [1.6679, 1.5671, 1.0231,  ..., 1.7735, 1.2046, 1.9898],\n",
       "        ...,\n",
       "        [1.3863, 1.5770, 1.1810,  ..., 1.9798, 1.5666, 1.6019],\n",
       "        [1.0903, 1.0175, 1.5882,  ..., 1.2834, 1.6954, 1.9360],\n",
       "        [1.9186, 1.0125, 1.4146,  ..., 1.8035, 1.9133, 1.5419]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.from_numpy(X_on_disk[0])\n",
    "x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As you can see, this doesn't affect the original np.memmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([[0.2626543 , 0.81090355, 0.32464218, ..., 0.23839498, 0.67757344,\n",
       "         0.3310132 ],\n",
       "        [0.7845197 , 0.68123066, 0.38421404, ..., 0.5383686 , 0.01147997,\n",
       "         0.13235402],\n",
       "        [0.6679299 , 0.56707335, 0.02308166, ..., 0.77351165, 0.20462573,\n",
       "         0.9898304 ],\n",
       "        ...,\n",
       "        [0.38630033, 0.5770135 , 0.18095827, ..., 0.9798217 , 0.56657016,\n",
       "         0.6018766 ],\n",
       "        [0.09030128, 0.01747155, 0.5882474 , ..., 0.28344667, 0.6954193 ,\n",
       "         0.93599343],\n",
       "        [0.91863537, 0.01252496, 0.4146458 , ..., 0.80348647, 0.9133351 ,\n",
       "         0.5418618 ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_on_disk[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You can slice an array on disk by any axis, and it'll return a memmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([[0.2626543 , 0.81090355, 0.32464218, ..., 0.23839498, 0.67757344,\n",
       "         0.3310132 ],\n",
       "        [0.7845197 , 0.68123066, 0.38421404, ..., 0.5383686 , 0.01147997,\n",
       "         0.13235402],\n",
       "        [0.6679299 , 0.56707335, 0.02308166, ..., 0.77351165, 0.20462573,\n",
       "         0.9898304 ],\n",
       "        ...,\n",
       "        [0.38630033, 0.5770135 , 0.18095827, ..., 0.9798217 , 0.56657016,\n",
       "         0.6018766 ],\n",
       "        [0.09030128, 0.01747155, 0.5882474 , ..., 0.28344667, 0.6954193 ,\n",
       "         0.93599343],\n",
       "        [0.91863537, 0.01252496, 0.4146458 , ..., 0.80348647, 0.9133351 ,\n",
       "         0.5418618 ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_on_disk[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([[0.2626543 , 0.81090355, 0.32464218, ..., 0.23839498, 0.67757344,\n",
       "         0.3310132 ],\n",
       "        [0.7720094 , 0.21011464, 0.02604653, ..., 0.8257726 , 0.9297855 ,\n",
       "         0.34065437],\n",
       "        [0.10447659, 0.7670673 , 0.838875  , ..., 0.76260966, 0.5328985 ,\n",
       "         0.2714968 ],\n",
       "        ...,\n",
       "        [0.8387722 , 0.13451461, 0.8197776 , ..., 0.3349404 , 0.43819886,\n",
       "         0.65123564],\n",
       "        [0.20458107, 0.76076484, 0.5841517 , ..., 0.8807168 , 0.8641069 ,\n",
       "         0.5569748 ],\n",
       "        [0.6686797 , 0.9680852 , 0.04992276, ..., 0.99571806, 0.22515585,\n",
       "         0.9119718 ]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_on_disk[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "However, bear in mind that if you use multiple indices, the output will be a regular numpy array. This is important as it will use more RAM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.2626543 , 0.81090355, 0.32464218, ..., 0.23839498,\n",
       "         0.67757344, 0.3310132 ],\n",
       "        [0.7845197 , 0.68123066, 0.38421404, ..., 0.5383686 ,\n",
       "         0.01147997, 0.13235402],\n",
       "        [0.6679299 , 0.56707335, 0.02308166, ..., 0.77351165,\n",
       "         0.20462573, 0.9898304 ],\n",
       "        ...,\n",
       "        [0.38630033, 0.5770135 , 0.18095827, ..., 0.9798217 ,\n",
       "         0.56657016, 0.6018766 ],\n",
       "        [0.09030128, 0.01747155, 0.5882474 , ..., 0.28344667,\n",
       "         0.6954193 , 0.93599343],\n",
       "        [0.91863537, 0.01252496, 0.4146458 , ..., 0.80348647,\n",
       "         0.9133351 , 0.5418618 ]],\n",
       "\n",
       "       [[0.7720094 , 0.21011464, 0.02604653, ..., 0.8257726 ,\n",
       "         0.9297855 , 0.34065437],\n",
       "        [0.7020411 , 0.7310075 , 0.00217595, ..., 0.74274397,\n",
       "         0.85443044, 0.469761  ],\n",
       "        [0.7890441 , 0.29349124, 0.4332237 , ..., 0.22531688,\n",
       "         0.13641207, 0.955273  ],\n",
       "        ...,\n",
       "        [0.08417109, 0.47016835, 0.9377146 , ..., 0.82218635,\n",
       "         0.58684355, 0.44834593],\n",
       "        [0.15103178, 0.53963864, 0.3386973 , ..., 0.9789423 ,\n",
       "         0.16320346, 0.75033426],\n",
       "        [0.9836441 , 0.8263102 , 0.97352153, ..., 0.9199377 ,\n",
       "         0.65910494, 0.63731635]]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_on_disk[[0,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There's a trick we can use avoid this making use of the excellent new L class in fastai. It is to **itemify** the np.memmap/s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def itemify(*x): return L(*x).zip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To itemify one or several np.memmap/s is very fast. Let's see how long it takes with a 10 GB array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_on_disk_as_items = itemify(X_on_disk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Less than 2 seconds! And you only need to perform this once!\n",
    "\n",
    "So now, you can select multiple items at the same time, and they will all still be on disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [(memmap([[0.2626543 , 0.81090355, 0.32464218, ..., 0.23839498, 0.67757344,\n",
       "         0.3310132 ],\n",
       "        [0.7845197 , 0.68123066, 0.38421404, ..., 0.5383686 , 0.01147997,\n",
       "         0.13235402],\n",
       "        [0.6679299 , 0.56707335, 0.02308166, ..., 0.77351165, 0.20462573,\n",
       "         0.9898304 ],\n",
       "        ...,\n",
       "        [0.38630033, 0.5770135 , 0.18095827, ..., 0.9798217 , 0.56657016,\n",
       "         0.6018766 ],\n",
       "        [0.09030128, 0.01747155, 0.5882474 , ..., 0.28344667, 0.6954193 ,\n",
       "         0.93599343],\n",
       "        [0.91863537, 0.01252496, 0.4146458 , ..., 0.80348647, 0.9133351 ,\n",
       "         0.5418618 ]], dtype=float32),),(memmap([[0.7720094 , 0.21011464, 0.02604653, ..., 0.8257726 , 0.9297855 ,\n",
       "         0.34065437],\n",
       "        [0.7020411 , 0.7310075 , 0.00217595, ..., 0.74274397, 0.85443044,\n",
       "         0.469761  ],\n",
       "        [0.7890441 , 0.29349124, 0.4332237 , ..., 0.22531688, 0.13641207,\n",
       "         0.955273  ],\n",
       "        ...,\n",
       "        [0.08417109, 0.47016835, 0.9377146 , ..., 0.82218635, 0.58684355,\n",
       "         0.44834593],\n",
       "        [0.15103178, 0.53963864, 0.3386973 , ..., 0.9789423 , 0.16320346,\n",
       "         0.75033426],\n",
       "        [0.9836441 , 0.8263102 , 0.97352153, ..., 0.9199377 , 0.65910494,\n",
       "         0.63731635]], dtype=float32),)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_on_disk_as_items[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Slicing is very fast, even if there are 100.000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.5 µs ± 238 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# axis 0\n",
    "%timeit X_on_disk[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.4 µs ± 139 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# axis 1\n",
    "%timeit X_on_disk[..., 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1 µs ± 233 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# axis 2\n",
    "%timeit X_on_disk[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.9 µs ± 156 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# aixs 0,1\n",
    "%timeit X_on_disk[0, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To compare how fast you can slice a np.memmap, let's create a smaller array that I can fit in memory (X_in_memory). This is 10 times smaller (100 MB) than the one on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_in_memory_small = np.random.rand(10000, 50, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "678 ns ± 55.9 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit X_in_memory_small[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's create the same array on disk. It's super simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.save('X_on_disk_small.npy', X_in_memory_small)\n",
    "X_on_disk_small = np.load('X_on_disk_small.npy', mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.6 µs ± 410 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit X_on_disk_small[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is 17 slower than having arrays on disk, although it's still pretty fast.\n",
    "\n",
    "However, if we use the itemified version, it's much faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.29 µs ± 6.21 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit X_on_disk_as_items[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is much better! So now you can access 1 of multiple items on disk with a pretty good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Calculating stats: mean and std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Another benefit of using arrays on disk is that you can calculate the mean and std deviation of the entire dataset. \n",
    "\n",
    "It takes a considerable time since the array is very big (10GB), but it's feasible:\n",
    "\n",
    "- mean (0.4999966):  1 min 45 s\n",
    "- std  (0.2886839): 11 min 43 s \n",
    "\n",
    "in my laptop. \n",
    "If you need them, you could calculate these stats once, and store the results (similar to ImageNet stats).\n",
    "However, you usually need to claculate these metrics for labeled (train) datasets, that tend to be smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4999966"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_mean = X_on_disk.mean()\n",
    "# X_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28868386"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_std = X_on_disk.std()\n",
    "# X_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Conversion into a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Conversion from an array on disk slice into a tensor is also very fast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2627, 0.8109, 0.3246,  ..., 0.2384, 0.6776, 0.3310],\n",
       "        [0.7845, 0.6812, 0.3842,  ..., 0.5384, 0.0115, 0.1324],\n",
       "        [0.6679, 0.5671, 0.0231,  ..., 0.7735, 0.2046, 0.9898],\n",
       "        ...,\n",
       "        [0.3863, 0.5770, 0.1810,  ..., 0.9798, 0.5666, 0.6019],\n",
       "        [0.0903, 0.0175, 0.5882,  ..., 0.2834, 0.6954, 0.9360],\n",
       "        [0.9186, 0.0125, 0.4146,  ..., 0.8035, 0.9133, 0.5419]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(X_on_disk[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_on_disk_small_0 = X_on_disk_small[0]\n",
    "X_in_memory_small_0 = X_in_memory_small[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.3 µs ± 272 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit torch.from_numpy(X_on_disk_small_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.6 µs ± 502 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit torch.from_numpy(X_in_memory_small_0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So it takes the same time to convert from numpy.memmap or from a np.array in memory is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Combined operations: slicing plus conversion to tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's now check performance of the combined process: slicing plus conversion to a tensor. Based on what we've seen there are 3 options: \n",
    "\n",
    "- slice np.array in memory + conversion to tensor\n",
    "- slice np.memamap on disk + conversion to tensor\n",
    "- slice itemified np.memmap + converion to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 µs ± 2.55 µs per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit torch.from_numpy(X_in_memory_small[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.2 µs ± 3.73 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit torch.from_numpy(X_on_disk_small[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_on_disk_small_as_items = itemify(X_on_disk_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.9 µs ± 271 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit torch.from_numpy(X_on_disk_small_as_items[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So this last method is **as fast as having the array in memory**!! This is an excellent outcome, since slicing arrays in memory is a highly optimized operation. \n",
    "\n",
    "And we have the benefit of having access to very large datasets if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We now have a very efficient way to work with very large numpy arrays.\n",
    "\n",
    "The process is very simple:\n",
    "\n",
    "- create and save the array on disk (as described before)\n",
    "- load it with a mmap_mode='r'\n",
    "- itemify the array/s\n",
    "\n",
    "So my recommendation would be:\n",
    "\n",
    "- use numpy arrays in memory when possible (if your data fits in memory)\n",
    "- use numpy memmap (arrays on disk) when data doesn't fit. You will still have a great performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
